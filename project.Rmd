---
title: "Machine Learning Project"
author: "James Clarke"
date: "Tuesday, May 12, 2015"
output: html_document
---

# Summary
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

This report describes an algorithm that attempts to predict how well the exercise was performed using data from accelerometers on the belt, forearm, arm, and dumbell.

More information about the source data is available here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

The algorithm presented here uses an ensemble classifier to give an estimated accuracy of 80% on previously unseen data.

The algorithm correctly classified x/20 samples from the the validation data set.

# Data
The algorithm was trained and evaluated using this data set:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The final grading of the algorithm was performed using this data set:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

# Method

## Load and clean data
The data is loaded and columns with missing data are dropped.
```{r, warning=F}
library(caret)
srcdata <- read.csv("pml-training.csv")
dropNa <- srcdata[,colSums(is.na(srcdata) | srcdata =="") == 0]
set.seed(98562)
```

## Train models
The models are trained using 75% of the data, with 25% saved for cross validation.
```{r, warning=F, results='hide'}
inTrain = createDataPartition(dropNa$classe, p = 0.75, list=F)
training = dropNa[inTrain,]
testing = dropNa[-inTrain,]
```

### Random Forests
```{r, warning=F, results='hide'}
mod1 <- train(classe~.,method="rf",preProcess = c("center","scale","pca"),training)
```

### Boosted Trees
```{r, warning=F, results='hide'}
mod2 <- train(classe~.,method="gbm",preProcess = c("center","scale","pca"),training)
```

### Linear Discriminant Analysis
```{r, warning=F, results='hide'}
mod3 <- train(classe~.,method="lda",preProcess = c("center","scale","pca"),training)
```

### Ensemble Model
```{r, warning=F, results='hide'}
pred1 <- predict(mod1,testing)
pred2 <- predict(mod2,testing)
pred3 <- predict(mod3,testing)
combDf = data.frame(pred1,pred2,pred3,classe=testing$classe)
mod <- train(classe~.,method="rf",combDf)
pred <- predict(mod,testing)
```

## Evaluate model
The model is evaluated using the 25% of the data that was set aside.
```{r warning=F}
confusionMatrix(testing$classe, pred)
oosError <- round((1 - confusionMatrix(testing$classe, pred)$overall[1]) * 100,0)
```
This suggests the out of sample error is `r oosError`%.

# Appendix

## Run model against grading test
The model is run against the grading test to generate the data for submission.
```{r, warning=F}
graddata <- read.csv("pml-testing.csv")
gradDropNa <- graddata[,colSums(is.na(graddata) | graddata =="") == 0]
gradDropNa <- gradDropNa[,1:length(gradDropNa)-1]
pred1 <- predict(mod1,gradDropNa)
pred2 <- predict(mod2,gradDropNa)
pred3 <- predict(mod3,gradDropNa)
combDf = data.frame(pred1,pred2,pred3)
predGrad <- predict(mod,combDf)
predDf <- data.frame(predGrad)
print(predDf)
```